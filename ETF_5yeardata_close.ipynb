{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import logging\n",
    "import random\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Activation, Flatten, Permute\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import LinearAnnealedPolicy, BoltzmannQPolicy, EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "from gym import error, spaces, utils\n",
    "from gym.utils import seeding\n",
    "import math\n",
    "import keras.backend as K\n",
    "from keras.layers import Input, Dense, concatenate, Lambda, Conv2D, Reshape\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.core import Env\n",
    "from rl.callbacks import FileLogger, ModelIntervalCheckpoint\n",
    "#read data as a pandas data frame(ACIM here)\n",
    "import pandas\n",
    "\n",
    "import os, sys, csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#找三十個有五年數據的資料\n",
    "ETF_datapath='/Users/fentingchen/desktop/A3C/ETF/etf_data'\n",
    "#ETF_datapath='/home/fei-tin/feiting/ETF_5year/etf_data'\n",
    "data_pathlist = os.listdir(ETF_datapath)[1:]\n",
    "start_date = '2011-11-16'\n",
    "data_path = []\n",
    "i=0\n",
    "while len(data_path) <30:\n",
    "    file = ETF_datapath + \"/\" + data_pathlist[i]\n",
    "    x = pd.read_csv(file)\n",
    "    y = x[x['Date'] == start_date]\n",
    "    if len(y) > 0:\n",
    "        data_path.append(file)\n",
    "    i+= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#整理data的函式\n",
    "def drop(data):\n",
    "    '''\n",
    "    input: etf dataframe\n",
    "    output: train dataframe, test dataframe, train date, test date\n",
    "    '''\n",
    "    start_date = '2012-02-14'\n",
    "    start_index = data[data['Date'] == start_date].index\n",
    "    data_new_reverse = data.iloc[start_index[0]::-1]\n",
    "    data_close = data_new_reverse[\"Close\"]\n",
    "    train_df = data_close.reset_index(drop=True)\n",
    "\n",
    "    return train_df\n",
    "\n",
    "def data(ETF_datapath):\n",
    "    test = []\n",
    "    train = []\n",
    "    vaild = []\n",
    "    x= [0]*20 + [1]*5 + [2]*5\n",
    "    random.shuffle(x)\n",
    "    for i in range(len(ETF_datapath)):\n",
    "        data_frame = pandas.read_csv(ETF_datapath[i])\n",
    "        data = drop(data_frame)\n",
    "        if x[i] == 0:\n",
    "                train.append(data.values.reshape(1260,1))\n",
    "        elif x[i] == 1:\n",
    "                test.append(data.values.reshape(1260,1))\n",
    "        elif x[i] == 2:\n",
    "                vaild.append(data.values.reshape(1260,1))\n",
    "    return train, test,vaild\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#整理資料\n",
    "data_train,data_test,data_vaild = data(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 30.02    ],\n",
       "       [ 30.17    ],\n",
       "       [ 30.450001],\n",
       "       ..., \n",
       "       [ 45.619999],\n",
       "       [ 45.450001],\n",
       "       [ 45.779999]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mode?test\n",
      "Step?1\n"
     ]
    }
   ],
   "source": [
    "#模式與訓練次數的選擇\n",
    "mode = input(\"Mode?\")\n",
    "step = int(input(\"Step?\"))\n",
    "number = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#準備當作輸入的資料\n",
    "class ETF_Game(Env):\n",
    "    def __init__(self):\n",
    "        self.action_space = spaces.Discrete(5)\n",
    "        self.observation_space = spaces.Box(low = 0, high = math.inf, shape = (20,1))\n",
    "        self.reward_range = (- math.inf, math.inf)\n",
    "\n",
    "        #隨便取個名字，方便我們存資料\n",
    "        self.name = \"GameETF\"\n",
    "\n",
    "        #設定隨機的seed\n",
    "        self.seed()\n",
    "\n",
    "        #重設遊戲\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.cycle = 0\n",
    "\n",
    "        #取得資料作為遊戲盤面\n",
    "        if mode == 'train':\n",
    "                i = random.randint(0,19)\n",
    "                self.board = data_train[i]\n",
    "        if mode == 'train_final':\n",
    "                self.board = data_train[number]\n",
    "        elif mode == 'test':\n",
    "                self.board = data_test[number]\n",
    "        #初始化日期(由於需要觀察之前的資料作為data，故從第20天開始)\n",
    "        self.day = 20\n",
    "\n",
    "        #最大天數 ＝ 矩陣的最大長度\n",
    "        self.max_day = self.board.shape[0]\n",
    "\n",
    "        #初始現金\n",
    "        self.cash = 20000\n",
    "        self.min_cash = 20000\n",
    "\n",
    "        #初始股票\n",
    "        self.num_of_stock = 0\n",
    "        self.financial_assets = 0\n",
    "        self.max_stock = 0\n",
    "        \n",
    "        #初始資產價值\n",
    "        self.assets = self.cash + self.financial_assets\n",
    "        self.assets_last_time = self.assets\n",
    "\n",
    "        #初始股票價格 \n",
    "        self.price = self.board[self.day-1][0]\n",
    "\n",
    "        #表示遊戲結束與否        \n",
    "        self.DONE = False\n",
    "\n",
    "        #回傳初始盤面（gym的規定）\n",
    "        return self.get_observation()\n",
    "\n",
    "    def step(self, action):\n",
    "\n",
    "        #動作會介在0, 1, 2 之間，分別設為買一單位，賣一單位，不動作。(測試)\n",
    "\n",
    "        # “現金” < \"當前股票價格“ 時，不得購買。\n",
    "        if 10*self.price <= self.cash < 20*self.price and action == 0:\n",
    "            action = 1\n",
    "        if self.cash < 10*self.price and (action == 1 or action == 0):\n",
    "            action = 2\n",
    "\n",
    "        # “股票數” = 0 時，不得賣出\n",
    "        if self.num_of_stock == 0 and (action == 3 or action == 4):\n",
    "            action = 2\n",
    "        if self.num_of_stock == 1 and action == 4:\n",
    "            action = 3\n",
    "\n",
    "        #紀錄\n",
    "        logging.debug(\"Action {}\".format(action))\n",
    "\n",
    "        #本次購買量\n",
    "        purchance_quantity = 0\n",
    "\n",
    "        # action\n",
    "        if action == 0:\n",
    "            purchance_quantity = 20\n",
    "            logging.debug(\"Buymore\")\n",
    "        elif action == 1:\n",
    "            purchance_quantity = 10\n",
    "            logging.debug(\"Buy\")\n",
    "        elif action == 2:\n",
    "            purchance_quantity = 0\n",
    "            logging.debug(\"Hold\")\n",
    "        elif action == 3:\n",
    "            purchance_quantity = -10\n",
    "            logging.debug(\"Sell\")\n",
    "        elif action == 4:\n",
    "            purchance_quantity = -20\n",
    "            logging.debug(\"Sellmore\")\n",
    "\n",
    "\n",
    "        #已購買的股票張數更動\n",
    "        self.num_of_stock += purchance_quantity\n",
    "        if self.max_stock < self.num_of_stock:\n",
    "            self.max_stock = self.num_of_stock\n",
    "\n",
    "        #現存股票價值更動\n",
    "        self.financial_assets = self.num_of_stock*self.price\n",
    "        #現金更動\n",
    "        self.cash -= purchance_quantity*self.price + 0.00145 * abs(purchance_quantity) * self.price\n",
    "        #交易稅的現今更動，只有賣出的需收取\n",
    "        if action == 3 or action == 4:\n",
    "            self.cash -= 0.001*abs(purchance_quantity)*self.price\n",
    "        \n",
    "        if self.min_cash < self.cash:\n",
    "            self.min_cash = self.cash\n",
    "        #總資產價值更動\n",
    "        self.assets = self.cash + self.financial_assets\n",
    "\n",
    "        #reward for this time\n",
    "        reward = self.assets - self.assets_last_time\n",
    "\n",
    "        #record today's asset\n",
    "        self.assets_last_time = self.assets\n",
    "\n",
    "        #週期數+1\n",
    "        self.cycle += 1\n",
    "        self.day += 20\n",
    "\n",
    "        #update股票價格 \n",
    "        self.price = self.board[self.day-1][0]\n",
    "\n",
    "        #get now observation for today\n",
    "        observation = self.get_observation()\n",
    "\n",
    "        #當自身資產歸零，或是到達最大天數時，遊戲結束，其餘均繼續進行。\n",
    "        done = None\n",
    "        if self.assets == 0 or self.day == self.max_day:\n",
    "            done = True\n",
    "            self.DONE = True\n",
    "        else:\n",
    "            done = False\n",
    "\n",
    "        #這裡可以回傳特定資料作為紀錄（格式是字典檔），因為我們目前沒有需要，所以隨便設個空的字典檔。\n",
    "        info = dict()\n",
    "\n",
    "        #這裡要回傳什麼，回傳的順序，都是gym規定的\n",
    "\n",
    "        return observation, reward, done, info\n",
    "\n",
    "\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]\n",
    "\n",
    "    def get_observation(self):\n",
    "        #get board for last 20 days\n",
    "        return self.board[self.day-20:self.day]\n",
    "\n",
    "    #這裡是拿來做test時候的顯示        \n",
    "    def render(self, mode='human', close=False):\n",
    "        if close:\n",
    "            return\n",
    "        outfile = None\n",
    "        if self.DONE == True:\n",
    "            outfile = StringIO() if mode == 'ansi' else sys.stdout\n",
    "            s = \"total assets:\" + str(self.assets) + \"\\n\"\n",
    "            s += \"min_cash:\" + str(self.min_cash) + \"\\n\"\n",
    "            #s += \"max_num_stock:\" + str(self.max_stock) + \"\\n\"\n",
    "            #s += \"cash left\" + str(self.cash) + \"\\n\"\n",
    "            #s += \"num of stock left:\" + str(self.num_of_stock) + \"\\n\"\n",
    "            #s += \"final price of ETF\" + str(self.price) + \"\\n\"\n",
    "            #s += \"financial_assets left\" + str(self.financial_assets) + \"\\n\"\n",
    "            outfile.write(s)\n",
    "        return outfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#把我們辛苦架好的遊戲環境作為測試環境\n",
    "ETF_env = ETF_Game()\n",
    "nb_actions = ETF_env.action_space.n\n",
    "\n",
    "#可以執行的動作數\n",
    "nb_actions\n",
    "BOARD_INPUT_SHAPE = (20, 1)\n",
    "WINDOW_LENGTH = 1\n",
    "\n",
    "#這裡的window_length 是指當我需要傳入包括前幾次畫面作為資料時的東西，\n",
    "#他是把它當作CNN的channel數一樣的東西\n",
    "#本來這裡是不需要加的，只是keras-rl寫死了所以我只好傳進去。\n",
    "\n",
    "#另外，由於資料最後一步是keras-rl處理的，他的順序這樣寫，\n",
    "#我也只好這樣寫\n",
    "input_shape = (WINDOW_LENGTH,) + BOARD_INPUT_SHAPE\n",
    "\n",
    "#設定輸入層的形狀\n",
    "model_input = Input(shape = input_shape)\n",
    "\n",
    "#視不同的backend要排一下順序\n",
    "if K.image_dim_ordering() == 'tf':\n",
    "    # (width, height, channels)\n",
    "    permute = Permute((2, 3, 1), input_shape=input_shape)\n",
    "elif K.image_dim_ordering() == 'th':\n",
    "    # (channels, width, height)\n",
    "    permute = Permute((1, 2, 3), input_shape=input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#把排列的結果套用上去，喬一下我們的原始input\n",
    "preprocessed_input = permute(model_input)\n",
    "\n",
    "flat_h_then_v = Flatten()(preprocessed_input)\n",
    "\n",
    "f = Dense(300 , activation = 'relu')\n",
    "x = f(flat_h_then_v)\n",
    "\n",
    "g = Dense(100 , activation = 'relu')\n",
    "y = g(x)\n",
    "\n",
    "\n",
    "action = Dense(5)\n",
    "output_action = action(y)\n",
    "\n",
    "model = Model(model_input, output_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Negative dimension size caused by subtracting 6 from 1 for 'conv2d_1/convolution' (op: 'Conv2D') with input shapes: [?,20,1,1], [1,6,1,32].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m/Users/fentingchen/anaconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/framework/common_shapes.py\u001b[0m in \u001b[0;36m_call_cpp_shape_fn_impl\u001b[0;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, debug_python_shape_fn, require_shape_fn)\u001b[0m\n\u001b[1;32m    670\u001b[0m           \u001b[0mgraph_def_version\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_def_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shapes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 671\u001b[0;31m           input_tensors_as_shapes, status)\n\u001b[0m\u001b[1;32m    672\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/fentingchen/anaconda/envs/py35/lib/python3.5/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/fentingchen/anaconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[0;34m()\u001b[0m\n\u001b[1;32m    465\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[1;32m    467\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Negative dimension size caused by subtracting 6 from 1 for 'conv2d_1/convolution' (op: 'Conv2D') with input shapes: [?,20,1,1], [1,6,1,32].",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-7c9a73428fc8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m#把處理過的input塞進去\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mlayer_horizontal_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconv_horizontal_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocessed_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mlayer_vertical_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconv_vertical_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocessed_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/fentingchen/anaconda/envs/py35/lib/python3.5/site-packages/keras/engine/topology.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m             \u001b[0;31m# Actually call the layer, collecting output(s), mask(s), and shape(s).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 554\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    555\u001b[0m             \u001b[0moutput_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprevious_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/fentingchen/anaconda/envs/py35/lib/python3.5/site-packages/keras/layers/convolutional.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    162\u001b[0m                 \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m                 \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m                 dilation_rate=self.dilation_rate)\n\u001b[0m\u001b[1;32m    165\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrank\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m             outputs = K.conv3d(\n",
      "\u001b[0;32m/Users/fentingchen/anaconda/envs/py35/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mconv2d\u001b[0;34m(x, kernel, strides, padding, data_format, dilation_rate)\u001b[0m\n\u001b[1;32m   2860\u001b[0m         \u001b[0mstrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstrides\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2861\u001b[0m         \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2862\u001b[0;31m         data_format='NHWC')\n\u001b[0m\u001b[1;32m   2863\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_postprocess_conv2d_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2864\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/fentingchen/anaconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36mconvolution\u001b[0;34m(input, filter, padding, strides, dilation_rate, name, data_format)\u001b[0m\n\u001b[1;32m    637\u001b[0m         \u001b[0mdilation_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdilation_rate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    638\u001b[0m         \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 639\u001b[0;31m         op=op)\n\u001b[0m\u001b[1;32m    640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/fentingchen/anaconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36mwith_space_to_batch\u001b[0;34m(input, dilation_rate, padding, op, filter_shape, spatial_dims)\u001b[0m\n\u001b[1;32m    306\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dilation_rate must be positive\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconst_rate\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 308\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_spatial_dims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m   \u001b[0;31m# We have two padding contributions. The first is used for converting \"SAME\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/fentingchen/anaconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36mop\u001b[0;34m(input_converted, _, padding)\u001b[0m\n\u001b[1;32m    629\u001b[0m           \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m           \u001b[0mstrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstrides\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 631\u001b[0;31m           name=name)\n\u001b[0m\u001b[1;32m    632\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m     return with_space_to_batch(\n",
      "\u001b[0;32m/Users/fentingchen/anaconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36m_non_atrous_convolution\u001b[0;34m(input, filter, padding, data_format, strides, name)\u001b[0m\n\u001b[1;32m    127\u001b[0m           \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m           \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m           name=name)\n\u001b[0m\u001b[1;32m    130\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mconv_dims\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mdata_format\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdata_format\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"NDHWC\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/fentingchen/anaconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/ops/gen_nn_ops.py\u001b[0m in \u001b[0;36mconv2d\u001b[0;34m(input, filter, strides, padding, use_cudnn_on_gpu, data_format, name)\u001b[0m\n\u001b[1;32m    394\u001b[0m                                 \u001b[0mstrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstrides\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m                                 \u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 396\u001b[0;31m                                 data_format=data_format, name=name)\n\u001b[0m\u001b[1;32m    397\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/fentingchen/anaconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36mapply_op\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    761\u001b[0m         op = g.create_op(op_type_name, inputs, output_types, name=scope,\n\u001b[1;32m    762\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 763\u001b[0;31m                          op_def=op_def)\n\u001b[0m\u001b[1;32m    764\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/fentingchen/anaconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mcreate_op\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device)\u001b[0m\n\u001b[1;32m   2327\u001b[0m                     original_op=self._default_original_op, op_def=op_def)\n\u001b[1;32m   2328\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcompute_shapes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2329\u001b[0;31m       \u001b[0mset_shapes_for_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2330\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2331\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_record_op_seen_by_control_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/fentingchen/anaconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mset_shapes_for_outputs\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m   1715\u001b[0m       \u001b[0mshape_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1716\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1717\u001b[0;31m   \u001b[0mshapes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshape_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1718\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mshapes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1719\u001b[0m     raise RuntimeError(\n",
      "\u001b[0;32m/Users/fentingchen/anaconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mcall_with_requiring\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m   1665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1666\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1667\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcall_cpp_shape_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequire_shape_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1668\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1669\u001b[0m   \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/fentingchen/anaconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/framework/common_shapes.py\u001b[0m in \u001b[0;36mcall_cpp_shape_fn\u001b[0;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, debug_python_shape_fn, require_shape_fn)\u001b[0m\n\u001b[1;32m    608\u001b[0m     res = _call_cpp_shape_fn_impl(op, input_tensors_needed,\n\u001b[1;32m    609\u001b[0m                                   \u001b[0minput_tensors_as_shapes_needed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m                                   debug_python_shape_fn, require_shape_fn)\n\u001b[0m\u001b[1;32m    611\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m       \u001b[0;31m# Handles the case where _call_cpp_shape_fn_impl calls unknown_shape(op).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/fentingchen/anaconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/framework/common_shapes.py\u001b[0m in \u001b[0;36m_call_cpp_shape_fn_impl\u001b[0;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, debug_python_shape_fn, require_shape_fn)\u001b[0m\n\u001b[1;32m    674\u001b[0m       \u001b[0mmissing_shape_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mmissing_shape_fn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Negative dimension size caused by subtracting 6 from 1 for 'conv2d_1/convolution' (op: 'Conv2D') with input shapes: [?,20,1,1], [1,6,1,32]."
     ]
    }
   ],
   "source": [
    "#把排列的結果套用上去，喬一下我們的原始input\n",
    "preprocessed_input = permute(model_input)\n",
    "\n",
    "#橫著看～\n",
    "conv_horizontal_1 =  Conv2D(filters = 32, kernel_size = (1, 6), padding='valid', activation = \"relu\")\n",
    "\n",
    "#直的看～\n",
    "conv_vertical_1 = Conv2D(filters = 32, kernel_size = (5, 1), padding='valid', activation = \"relu\")\n",
    "\n",
    "#把處理過的input塞進去\n",
    "layer_horizontal_1 = conv_horizontal_1(preprocessed_input)\n",
    "layer_vertical_1 = conv_vertical_1(preprocessed_input)\n",
    "\n",
    "#再橫著看\n",
    "conv_horizontal_2 =  Conv2D(filters = 64, kernel_size = (1, 6), padding='valid', activation = \"relu\")\n",
    "\n",
    "#在直著看\n",
    "conv_vertical_2 = Conv2D(filters = 64, kernel_size = (5, 1), padding='valid', activation = \"relu\")\n",
    "\n",
    "#交錯塞\n",
    "layer_h_then_v_2 = conv_vertical_2(layer_horizontal_1)\n",
    "layer_v_then_h_2 = conv_horizontal_2(layer_vertical_1)\n",
    "\n",
    "#把上面兩個拉直\n",
    "flat_h_then_v = Flatten()(layer_h_then_v_2)\n",
    "flat_v_then_h = Flatten()(layer_v_then_h_2)\n",
    "\n",
    "#接在一起～\n",
    "\n",
    "\n",
    "conv_merge = concatenate([flat_h_then_v, flat_v_then_h], name = \"Merge_Layer\")\n",
    "\n",
    "f = Dense(300 , activation = 'relu')\n",
    "x = f(conv_merge)\n",
    "\n",
    "g = Dense(100 , activation = 'relu')\n",
    "y = g(x)\n",
    "\n",
    "\n",
    "action = Dense(5)\n",
    "output_action = action(y)\n",
    "\n",
    "model = Model(model_input, output_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preprocessed_input = permute(model_input)\n",
    "'''\n",
    "#橫著看～\n",
    "conv_horizontal_1 =  Conv2D(filters = 32, kernel_size = (3, 3), padding='valid', activation = \"relu\")\n",
    "\n",
    "#直的看～\n",
    "conv_vertical_1 = Conv2D(filters = 32, kernel_size = (3, 3), padding='valid', activation = \"relu\")\n",
    "\n",
    "#把處理過的input塞進去\n",
    "layer_horizontal_1 = conv_horizontal_1(preprocessed_input)\n",
    "layer_vertical_1 = conv_vertical_1(preprocessed_input)\n",
    "'''\n",
    "#再橫著看\n",
    "conv_horizontal_2 =  Conv2D(filters = 64, kernel_size = (2, 2), padding='valid', activation = \"relu\")\n",
    "\n",
    "cnn1 = conv_horizontal_2(preprocessed_input)\n",
    "#在直著看\n",
    "conv_vertical_2 = Conv2D(filters = 64, kernel_size = (3, 3), padding='valid', activation = \"relu\")\n",
    "\n",
    "cnn2 = conv_vertical_2(cnn1)\n",
    "conv_merge = Flatten()(cnn2)\n",
    "'''\n",
    "#交錯塞\n",
    "layer_h_then_v_2 = conv_vertical_2(layer_horizontal_1)\n",
    "layer_v_then_h_2 = conv_horizontal_2(layer_vertical_1)\n",
    "\n",
    "#把上面兩個拉直\n",
    "flat_h_then_v = Flatten()(layer_h_then_v_2)\n",
    "flat_v_then_h = Flatten()(layer_v_then_h_2)\n",
    "\n",
    "#接在一起～\n",
    "\n",
    "\n",
    "conv_merge = concatenate([flat_h_then_v, flat_v_then_h], name = \"Merge_Layer\")\n",
    "'''\n",
    "f = Dense(300 , activation = 'relu')\n",
    "x = f(conv_merge)\n",
    "\n",
    "g = Dense(100 , activation = 'relu')\n",
    "y = g(x)\n",
    "\n",
    "\n",
    "action = Dense(5)\n",
    "output_action = action(y)\n",
    "\n",
    "model = Model(model_input, output_action)\n",
    "\n",
    "#設定記憶體\n",
    "memory = SequentialMemory(limit=10000, window_length=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#設定記憶體\n",
    "memory = SequentialMemory(limit=10000, window_length=1)\n",
    "\n",
    "#set policy\n",
    "policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), attr='eps', value_max=1.,value_min=0, value_test=0, nb_steps=step - 1000000)\n",
    "\n",
    "#DQN設定\n",
    "dqn = DQNAgent(model=model, nb_actions=nb_actions, memory = memory, policy = policy,\n",
    "               nb_steps_warmup=100000, gamma=.95, target_model_update=50,\n",
    "               train_interval=1, delta_clip=1.)\n",
    "dqn.compile(Adam(lr=.00001), metrics=['mae'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights:  dqn_ETF_weights2_300_100.h5f\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "You are trying to load a weight file containing 7 layers into a model with 3 layers.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-88fea195dd78>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mweights_filename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mdqn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'test'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mdata_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/fentingchen/anaconda/envs/py35/lib/python3.5/site-packages/rl/agents/dqn.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_target_model_hard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/fentingchen/anaconda/envs/py35/lib/python3.5/site-packages/keras/engine/topology.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name)\u001b[0m\n\u001b[1;32m   2493\u001b[0m             \u001b[0mload_weights_from_hdf5_group_by_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2494\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2495\u001b[0;31m             \u001b[0mload_weights_from_hdf5_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2496\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2497\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'close'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/fentingchen/anaconda/envs/py35/lib/python3.5/site-packages/keras/engine/topology.py\u001b[0m in \u001b[0;36mload_weights_from_hdf5_group\u001b[0;34m(f, layers)\u001b[0m\n\u001b[1;32m   2879\u001b[0m                          \u001b[0;34m'containing '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2880\u001b[0m                          \u001b[0;34m' layers into a model with '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2881\u001b[0;31m                          str(len(filtered_layers)) + ' layers.')\n\u001b[0m\u001b[1;32m   2882\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2883\u001b[0m     \u001b[0;31m# We batch weight value assignments in a single backend call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: You are trying to load a weight file containing 7 layers into a model with 3 layers."
     ]
    }
   ],
   "source": [
    "if mode == 'train':\n",
    "    # Okay, now it's time to learn something! We capture the interrupt exception so that training\n",
    "    # can be prematurely aborted. Notice that you can the built-in Keras callbacks!\n",
    "    weights_filename = 'dqn_{}_weights.h5f'.format(ETF_env.name)\n",
    "    checkpoint_weights_filename = 'dqn_' + ETF_env.name + '_weights2_{step}.h5f'\n",
    "    log_filename = 'dqn_{}_log.json'.format(ETF_env.name)\n",
    "    callbacks = [ModelIntervalCheckpoint(checkpoint_weights_filename, interval=100000)]\n",
    "    callbacks += [FileLogger(log_filename, interval=100)]\n",
    "\n",
    "    #load weights\n",
    "    #dqn.load_weights(\"dqn_Game2048Env_weights2_10000000.h5f\")\n",
    "\n",
    "    dqn.fit(ETF_env, callbacks=callbacks, nb_steps=step, log_interval=10000)\n",
    "\n",
    "    # After training is done, we save the final weights one more time.\n",
    "    dqn.save_weights(weights_filename, overwrite=True)\n",
    "\n",
    "    # Finally, evaluate our algorithm for 10 episodes.\n",
    "    #dqn.test(env, nb_episodes=10, visualize=False)\n",
    "\n",
    "elif mode == 'test' or mode =='train_final':\n",
    "    #weights = \"dqn_\"+ETF_env.name+\"_weights2_10000000.h5f\"\n",
    "    weights = \"dqn_ETF_weights2_300_100.h5f\"\n",
    "    print(\"weights: \", weights)\n",
    "    if weights:\n",
    "        weights_filename = weights\n",
    "    dqn.load_weights(weights_filename)\n",
    "    if mode == 'test':\n",
    "        data_len = len(data_test)\n",
    "    if mode == 'train_final':\n",
    "        data_len = len(data_train)\n",
    "    for i in range(data_len):\n",
    "        number = i\n",
    "        dqn.test(ETF_env, nb_episodes=1, visualize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    if i == 3:\n",
    "        continue\n",
    "    print (i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54\n"
     ]
    }
   ],
   "source": [
    "ETF_datapath='/Users/fentingchen/desktop/A3C/ETF/etf_data'\n",
    "#ETF_datapath='/home/fei-tin/feiting/ETF_5year/etf_data'\n",
    "data_pathlist = os.listdir(ETF_datapath)[1:]\n",
    "data_pathlist[2]=\"GSD.csv\"\n",
    "start_date = '2011-11-16'\n",
    "data_path = []\n",
    "i=0\n",
    "while len(data_path) < 30:\n",
    "    try:\n",
    "        file = ETF_datapath + \"/\" + data_pathlist[i]\n",
    "        x = pd.read_csv(file)\n",
    "        y = x[x['Date'] == start_date]\n",
    "        if len(y) > 0:\n",
    "            data_path.append(file)\n",
    "\n",
    "        i+= 1\n",
    "    except:\n",
    "        i+= 1\n",
    "        continue\n",
    "print (i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/fentingchen/desktop/A3C/ETF/etf_data/AADR.csv',\n",
       " '/Users/fentingchen/desktop/A3C/ETF/etf_data/AAXJ.csv',\n",
       " '/Users/fentingchen/desktop/A3C/ETF/etf_data/ACWI.csv',\n",
       " '/Users/fentingchen/desktop/A3C/ETF/etf_data/ACWV.csv',\n",
       " '/Users/fentingchen/desktop/A3C/ETF/etf_data/ACWX.csv',\n",
       " '/Users/fentingchen/desktop/A3C/ETF/etf_data/ADRA.csv',\n",
       " '/Users/fentingchen/desktop/A3C/ETF/etf_data/ADRD.csv',\n",
       " '/Users/fentingchen/desktop/A3C/ETF/etf_data/ADRE.csv',\n",
       " '/Users/fentingchen/desktop/A3C/ETF/etf_data/ADRU.csv',\n",
       " '/Users/fentingchen/desktop/A3C/ETF/etf_data/ADZ.csv',\n",
       " '/Users/fentingchen/desktop/A3C/ETF/etf_data/AFK.csv',\n",
       " '/Users/fentingchen/desktop/A3C/ETF/etf_data/AGA.csv',\n",
       " '/Users/fentingchen/desktop/A3C/ETF/etf_data/AGF.csv',\n",
       " '/Users/fentingchen/desktop/A3C/ETF/etf_data/AGG.csv',\n",
       " '/Users/fentingchen/desktop/A3C/ETF/etf_data/AGQ.csv',\n",
       " '/Users/fentingchen/desktop/A3C/ETF/etf_data/AGZ.csv',\n",
       " '/Users/fentingchen/desktop/A3C/ETF/etf_data/AIA.csv',\n",
       " '/Users/fentingchen/desktop/A3C/ETF/etf_data/ALD.csv',\n",
       " '/Users/fentingchen/desktop/A3C/ETF/etf_data/AMJ.csv',\n",
       " '/Users/fentingchen/desktop/A3C/ETF/etf_data/AMLP.csv',\n",
       " '/Users/fentingchen/desktop/A3C/ETF/etf_data/AND.csv',\n",
       " '/Users/fentingchen/desktop/A3C/ETF/etf_data/AOA.csv',\n",
       " '/Users/fentingchen/desktop/A3C/ETF/etf_data/AOK.csv',\n",
       " '/Users/fentingchen/desktop/A3C/ETF/etf_data/AOM.csv',\n",
       " '/Users/fentingchen/desktop/A3C/ETF/etf_data/AOR.csv',\n",
       " '/Users/fentingchen/desktop/A3C/ETF/etf_data/ARGT.csv',\n",
       " '/Users/fentingchen/desktop/A3C/ETF/etf_data/ASEA.csv',\n",
       " '/Users/fentingchen/desktop/A3C/ETF/etf_data/AUNZ.csv',\n",
       " '/Users/fentingchen/desktop/A3C/ETF/etf_data/AUSE.csv',\n",
       " '/Users/fentingchen/desktop/A3C/ETF/etf_data/AXJL.csv']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_path = '/Users/fentingchen/desktop/A3C/ETF/etf_data/HEFA.csv'\n",
    "x = pd.read_csv(X_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>b'Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Adj Close\\n2017-02-15</th>\n",
       "      <th>26.85</th>\n",
       "      <th>26.959999</th>\n",
       "      <th>26.85.1</th>\n",
       "      <th>...</th>\n",
       "      <th>24.709999.26</th>\n",
       "      <th>24.709999.27</th>\n",
       "      <th>1700.1</th>\n",
       "      <th>22.408763\\n2014-02-14</th>\n",
       "      <th>24.52.15</th>\n",
       "      <th>24.52.16</th>\n",
       "      <th>24.52.17</th>\n",
       "      <th>24.52.18</th>\n",
       "      <th>1300.2</th>\n",
       "      <th>22.236459\\n'</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 4549 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [b'Date, Open, High, Low, Close, Volume, Adj Close\\n2017-02-15, 26.85, 26.959999, 26.85.1, 26.940001, 848900, 26.940001\\n2017-02-14, 26.76, 26.889999, 26.74, 26.889999.1, 1758300, 26.889999\\n2017-02-13, 26.870001, 26.90, 26.84, 26.85.2, 436000, 26.85\\n2017-02-10, 26.66, 26.73, 26.639999, 26.709999, 657100, 26.709999\\n2017-02-09, 26.42, 26.620001, 26.42.1, 26.60, 560500, 26.60\\n2017-02-08, 26.299999, 26.40, 26.23, 26.389999, 929200, 26.389999\\n2017-02-07, 26.360001, 26.42.2, 26.280001, 26.360001.1, 1150700, 26.360001\\n2017-02-06, 26.299999.1, 26.33, 26.219999, 26.27, 954000, 26.27\\n2017-02-03, 26.48, 26.51, 26.42.3, 26.50, 789500, 26.50\\n2017-02-02, 26.33.1, 26.40.1, 26.27.1, 26.370001, 930600, 26.370001\\n2017-02-01, 26.50.1, 26.50.2, 26.34, 26.389999.1, 883200, 26.389999\\n2017-01-31, 26.290001, 26.32, 26.129999, 26.24, 2459800, 26.24\\n2017-01-30, 26.459999, 26.459999.1, 26.290001.1, 26.40.2, 938100, 26.40\\n2017-01-27, 26.67, 26.68, 26.610001, 26.67.1, 1020000, 26.67\\n2017-01-26, 26.73.1, 26.75, 26.66.1, 26.67.2, 1184700, 26.67\\n2017-01-25, 26.60.1, 26.639999.1, 26.559999, ...]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 4549 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ETF_datapath='/Users/fentingchen/desktop/A3C/ETF/etf_data'\n",
    "data_pathlist = os.listdir(ETF_datapath)[1:]\n",
    "start_date = '2011-11-16'\n",
    "data_path = []\n",
    "data_name = []\n",
    "i=0\n",
    "\n",
    "while len(data_path) <30:\n",
    "    try:\n",
    "        file = ETF_datapath + \"/\" + data_pathlist[i]\n",
    "        x = pd.read_csv(file)\n",
    "        y = x[x['Date'] == start_date]\n",
    "        if len(y) > 0:\n",
    "            data_path.append(file)\n",
    "            data_name.append(data_pathlist[i])\n",
    "        i+= 1\n",
    "    except:\n",
    "        i+= 1\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [py35]",
   "language": "python",
   "name": "Python [py35]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
